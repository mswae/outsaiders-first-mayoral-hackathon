{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Da1iH4Ub7Dv2hQtfw9Vi4bcU-NHKa1U2",
      "authorship_tag": "ABX9TyOBl/cc4b7RfhG3iq3lrBN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mswae/first-mayoral-hackathon/blob/main/peopulse.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Peopulse**: Public Concern Early Warning System and Decision Support"
      ],
      "metadata": {
        "id": "gg-dNCCLA34p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "izyi_vTFBSYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets evaluate\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re #regular expression; for text manipulation\n",
        "import evaluate #for evaluation metrics\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import hdbscan"
      ],
      "metadata": {
        "id": "hVAa4ZydA6ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset"
      ],
      "metadata": {
        "id": "-Y4K3kHmKeJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ds_name = \"/content/drive/MyDrive/Datasets/populse_dataset.csv\"\n",
        "df = pd.read_csv(ds_name)"
      ],
      "metadata": {
        "id": "0sEnocwcKdA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model from Hugging Face"
      ],
      "metadata": {
        "id": "10B4CM90ONnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loads RoBERTa-Tagalog\n",
        "model_name = \"jcblaise/roberta-tagalog-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_rtb = AutoModelForSequenceClassification.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "hGIpnhzhORRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Helper Functions"
      ],
      "metadata": {
        "id": "uDL8wiIsK5Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- preprocessing helper functions ---\n",
        "\n",
        "def clean_data(text):\n",
        "\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"http\\S+|www\\S+\", \"\", text)  #remove URLs\n",
        "  text = re.sub(r\"@\\w+\", \"\", text)  #remove @mentions\n",
        "  text = re.sub(r\"u/\\w+\", \"\", text)  #remove u/mentions\n",
        "  text = re.sub(r\"#(\\w+)\", r\"\\1\", text)  #remove hashtag symbols but keep the word\n",
        "  text = re.sub(r\"[^\\w\\s!?.,'’“”-]\", \" \", text)  #keep only basic punctuation\n",
        "  text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "  return text\n",
        "\n",
        "def tokenize_fn(batch, tokenizer, text_col=\"text\", label_col=\"category\"):\n",
        "\n",
        "  tokens = tokenizer(batch[text_col], truncation=True, padding=\"max_length\", max_length=128    )\n",
        "  tokens[\"labels\"] = batch[label_col]\n",
        "\n",
        "  return tokens\n",
        "\n",
        "# --- evaluation helper function ---\n",
        "\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    precision = precision_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"recall\"]\n",
        "\n",
        "    results = {\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "    }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "tNpt9GdlK-zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratify Data for Training and Testing (80-20)"
      ],
      "metadata": {
        "id": "cYokZAiDBhL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_col = 'category' #replace with the actual name of your label column\n",
        "\n",
        "X = df.drop(columns=[label_col])\n",
        "y = df[label_col]\n",
        "\n",
        "#perform an 80-20 stratified train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Original dataset shape: {y.value_counts(normalize=True)}\")\n",
        "print(f\"Train set shape: {y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Test set shape: {y_test.value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "id": "SJXr4n24ByWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "QUVN2YurO_fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwPwywKHPCFf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}